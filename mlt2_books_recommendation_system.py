# -*- coding: utf-8 -*-
"""MLT2_Books_Recommendation_System.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12NwezXJnzVXOlM_n2kEa8NjLsJLt4Y9s

# Data Loading

## Import Library
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import warnings
warnings.filterwarnings('ignore')

"""## Mengambil Dataset dari Kaggle"""

#install kaggle
!pip install -q kaggle
from google.colab import files
files.upload() #upload file kaggle.json

!rm -r ~/.kaggle
!mkdir ~/.kaggle
!mv ./kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

"""**Unduh dataset**"""

#mengunduh dataset dari kaggle
!kaggle datasets download -d zygmunt/goodbooks-10k

#mengekstrak file zip dari kaggle
!unzip /content/goodbooks-10k.zip

"""# Data Understanding
Load data menggunakan pandas
"""

buku = pd.read_csv('/content/books.csv')
genre = pd.read_csv('/content/book_tags.csv')
jenis_genre = pd.read_csv('/content/tags.csv')
user = pd.read_csv('/content/to_read.csv')
rating = pd.read_csv('/content/ratings.csv')

"""**Dataset books**"""

buku.info()

buku.head(3)

buku.shape

print('Jumlah buku: ', len(buku.book_id.unique()))
print('Jumlah author: ', len(buku.authors.unique()))

"""**Dataset genre**"""

genre.info()

genre.head(3)

genre.shape

print('Jumlah genre buku: ', len(genre.tag_id.unique()))

"""**Dataset jenis_genre**"""

jenis_genre.info()

jenis_genre.head(3)

jenis_genre.shape

print('Jumlah jenis genre buku: ', len(jenis_genre.tag_name.unique()))

"""**Dataset ratings**"""

rating.info()

rating.head(3)

rating.shape

rate = rating.groupby('rating').count()
rate

plt.figure(figsize=(8,4))
plt.title('Jumlah Rating Buku dari Pengguna')
plt.xlabel('Rating')
plt.ylabel('Jumlah Buku')
plt.bar(rate.index, rate['book_id'])
plt.grid(True)
plt.show()

"""**Dataset users**"""

user.head(3)

user.info()

user.describe()

print('Jumlah user: ', len(user.user_id.unique()))

"""# Data Preprocessing

**Menggabungkan tagID pada Buku**
"""

tags = np.concatenate((
    genre.tag_id.unique(),
    jenis_genre.tag_id.unique()
))

#mengurutkan data dan menghapus data yang sama
tags = np.sort(np.unique(tags))

print('Jumlah genre berdasarkan tag_id: ', len(tags))

"""**Mencari Jumlah Rating**"""

books = pd.merge(rating, buku, on='book_id', how='left')
books

"""**Memeriksa Missing Value**"""

books.isnull().sum()

books.groupby('book_id').sum()

"""**Menggabungkan Data dengan Judul, Penulis, dan Tahun Terbit Buku**"""

rate = rating
rate

df = pd.merge(rating, books[['book_id', 'authors', 'title', 'original_publication_year']], on='book_id', how='left')
df

"""# Content Based Filtering

## Data Preparation

**Mengatasi Missing Value**
"""

df.isnull().sum()

"""Terdapat nilai kosong pada kolom `authors`, `title`, dan `original_publication_year`."""

#menghapus data yang bernilai kosong
df = df.dropna()

df

#missing value setelah menghapus data yg kosong
df.isnull().sum()

"""Setelah dilakukan penghapusan pada missing value, kini sudah tidak terdapat missing value pada data. Jumlah row yang awalnya 96.708.232 berkurang menjadi 7.837.915"""

print('Jumlah judul buku: ', len(df.title.unique()))
print('Jumlah author: ', len(df.authors.unique()))
print('Jumlah tahun publikasi buku: ', len(df.original_publication_year.unique()))

"""Menghapus data yang sama berdasarkan `book_id` supaya data lebih rapi"""

df = df.drop_duplicates('book_id')
df

#konversi 'book_id', 'title', 'authors', dan 'original_publication_year' menjadi bentuk list
book_id = df['book_id'].tolist()
book_ttl = df['title'].tolist()
book_auth = df['authors'].tolist()
book_publ = df['original_publication_year'].tolist()

#membuat dictionary dari data yg dikonversi diatas
df = pd.DataFrame({
    'IDBuku' : book_id,
    'judul_buku' : book_ttl,
    'penulis' : book_auth,
    'thn_terbit' : book_publ
})
df

"""## Model Development

**TF-IDF Vectorizer**
"""

from sklearn.feature_extraction.text import TfidfVectorizer

tf = TfidfVectorizer()
tf.fit(df['penulis'])
tf.get_feature_names_out()

#fitting dan transformasi ke bentuk matrix
tf_matrix = tf.fit_transform(df['penulis'])
tf_matrix.shape

tf_matrix.todense()

#membuat dataframe untuk melihat tf-idf matrix, kolom=penulis, baris=judul buku
pd.DataFrame(
    tf_matrix.todense(),
    columns=tf.get_feature_names_out(),
    index=df.judul_buku
).sample(22, axis=1).sample(10, axis=0)

"""**Cosine Similarity**"""

from sklearn.metrics.pairwise import cosine_similarity

#hitung cosine similarity pada matrix tf-idf
cos_sim = cosine_similarity(tf_matrix)
cos_sim

#membuat dataframe dari cos_sim dengan baris dan kolom berupa judul buku
cos_sim_df = pd.DataFrame(cos_sim, index=df['judul_buku'], columns=df['judul_buku'])
print('Shape:', cos_sim_df.shape)

# Melihat similarity matrix pada setiap judul buku
cos_sim_df.sample(5, axis=1).sample(10, axis=0)

"""## Mendapatkan Rekomendasi
Membuat fungsi `book_reccomendations()`
"""

def book_recommendations(judul_buku, similarity_data=cos_sim_df, items=df[['judul_buku', 'penulis']], k=5):
    index = similarity_data.loc[:,judul_buku].to_numpy().argpartition(
        range(-1, -k, -1))

    # Mengambil data dengan similarity terbesar dari index yang ada
    closest = similarity_data.columns[index[-1:-(k+2):-1]]

    # Drop nama_resto agar nama resto yang dicari tidak muncul dalam daftar rekomendasi
    closest = closest.drop(judul_buku, errors='ignore')

    return pd.DataFrame(closest).merge(items).head(k)

#data buku yang dicari
df[df.judul_buku.eq('The Door Into Summer')]

#hasil rekomendasi buku berdasarkan buku yang dicari
book_recommendations('The Door Into Summer')

"""Hasil rekomendasi menunjukkan buku dengan nama penulis sama atau hampir sama karena sistem rekomendasi dibuat berdasarkan penulis.

# Collaborative Filtering

## Data Understanding
"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from pathlib import Path
import matplotlib.pyplot as plt

dr = rating
dr

"""## Data Preparation"""

#mnegubah user_id menjadi list unique dan melakukan encoding
id_user = dr['user_id'].unique().tolist()
user_to_user_encoded = {x: i for i, x in enumerate(id_user)}
user_encoded_to_user = {i: x for i, x in enumerate(id_user)}

print(user_to_user_encoded)

#mengubah book_id menjadi list unique dan melakukan encoding
id_buku = dr['book_id'].unique().tolist()
buku_to_buku_encoded = {x: i for i, x in enumerate(id_buku)}
buku_encoded_to_buku = {i: x for i, x in enumerate(id_buku)}

# Mendapatkan jumlah readers
num_readers = len(user_to_user_encoded)
print(num_readers)

# Mendapatkan jumlah resto
num_books = len(buku_encoded_to_buku)
print(num_books)

# Mengubah rating menjadi nilai float
dr['rating'] = dr['rating'].values.astype(np.float32)

# Nilai minimum rating
min_rating = min(dr['rating'])

# Nilai maksimal rating
max_rating = max(dr['rating'])

print('Jumlah Pembaca: {}, Jumlah Buku: {}, Min Rating: {}, Max Rating: {}'.format(
    num_readers, num_books, min_rating, max_rating
))

"""**Membagi Data untuk Training dan Validasi**"""

# Mengacak dataset
dr = dr.sample(frac=1, random_state=42)
dr

# Membuat variabel x untuk mencocokkan data pembaca dan buku menjadi satu value
x = dr[['user_id', 'book_id']].values

# Membuat variabel y untuk membuat rating dari hasil
y = dr['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

# Membagi menjadi 80% data train dan 20% data validasi
train_indices = int(0.8 * dr.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x, y)

"""## Model Development"""

class RecommenderNet(tf.keras.Model):

  # Insialisasi fungsi
  def __init__(self, num_users, num_resto, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_readers = num_readers
    self.num_books = num_books
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_readers,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_readers, 1) # layer embedding user bias
    self.book_embedding = layers.Embedding( # layer embeddings resto
        num_books,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.book_bias = layers.Embedding(num_books, 1) # layer embedding resto bias

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2
    book_vector = self.book_embedding(inputs[:, 1]) # memanggil layer embedding 3
    book_bias = self.book_bias(inputs[:, 1]) # memanggil layer embedding 4

    dot_user_book = tf.tensordot(user_vector, book_vector, 2)

    x = dot_user_book + user_bias + book_bias

    return tf.nn.sigmoid(x) # activation sigmoid

"""# Training Model"""

model = RecommenderNet(num_readers, num_books, 50) # inisialisasi model

# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 8,
    steps_per_epoch = 10000,
    epochs = 15,
    validation_data = (x_val, y_val)
)

"""# Visualisasi Metrik

Mendapatkan Rekomendasi Collaborative Filtering
"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""# Rekomendasi Menggunakan Collaborative Filtering"""

book_df = df
data = pd.read_csv('/content/ratings.csv')

# Mengambil sample user
id_pembaca = data.user_id.sample(1).iloc[0]
book_read_by_user = data[data.user_id == id_pembaca]

# Operator bitwise (~), bisa diketahui di sini https://docs.python.org/3/reference/expressions.html
book_not_read = book_df[~book_df['IDBuku'].isin(book_read_by_user.book_id.values)]['IDBuku']
book_not_read = list(
    set(book_not_read)
    .intersection(set(buku_to_buku_encoded.keys()))
)

book_not_read = [[buku_to_buku_encoded.get(x)] for x in book_not_read]
user_encoder = user_to_user_encoded.get(id_pembaca)
user_book_array = np.hstack(
    ([[user_encoder]] * len(book_not_read), book_not_read)
)

rate_buku = model.predict(user_book_array).flatten()

top_ratings_indices = rate_buku.argsort()[-10:][::-1]
recommended_book_ids = [
    buku_encoded_to_buku.get(book_not_read[x][0]) for x in top_ratings_indices
]

print('Menampilkan Rekomendasi Buku untuk User ID: {}'.format(id_pembaca))
print('===' * 12)
print('Rekomendasi Buku dengan Rating Tinggi dari Pembaca')
print('----' * 12)

top_book_user = (
    book_read_by_user.sort_values(
        by = 'rating',
        ascending=False
    )
    .head(5)
    .book_id.values
)

book_df_rows = book_df[book_df['IDBuku'].isin(top_book_user)]
for row in book_df_rows.itertuples():
    print(row.penulis, ':', row.judul_buku)

print('----' * 12)
print('Top 10 Rekomendasi Buku')
print('----' * 12)

recommended_book = book_df[book_df['IDBuku'].isin(recommended_book_ids)]
for row in recommended_book.itertuples():
    print(row.penulis, ':', row.judul_buku)

